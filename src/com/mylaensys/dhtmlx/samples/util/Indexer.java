package com.mylaensys.dhtmlx.samples.util;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.Token;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.snowball.SnowballAnalyzer;

import java.io.IOException;
import java.io.StringReader;
import java.util.HashSet;
import java.util.Set;
import java.util.logging.Logger;

/**
 * Data indexer for full text search
 *
 * @author Madytyoo
 */
public class Indexer {
    private static final Logger log = Logger.getLogger(Indexer.class.getName());
    /** From StopAnalyzer Lucene 2.9.1 */
    public final static String[] stopWords = new String[]{
                      "a", "an", "and", "are", "as", "at", "be", "but", "by",
                      "for", "if", "in", "into", "is", "it",
                      "no", "not", "of", "on", "or", "such",
                      "that", "the", "their", "then", "there", "these",
                      "they", "this", "to", "was", "will", "with"
    };

    /**
     * Extract the keywords from a raw text
     * @param data data to extract indexes
     * @return a Set of keywords
     */
    public static Set<String> getTokensForIndexing(String data) {
        int maximumNumberOfTokensToReturn = 20;

        String indexCleanedOfHTMLTags = data.replaceAll("\\<.*?>", " ");
        Set<String> returnSet = new HashSet<String>();

        try {

            Analyzer analyzer = new SnowballAnalyzer(org.apache.lucene.util.Version.LUCENE_CURRENT,"English",stopWords);
            TokenStream tokenStream = analyzer.tokenStream("content", new StringReader(indexCleanedOfHTMLTags));
            Token token = new Token();
            while (((token = tokenStream.next()) != null)&& (returnSet.size() < maximumNumberOfTokensToReturn)) {
                returnSet.add(token.term());
            }

        } catch (IOException e) {
            log.severe(e.getMessage());
        }

        return returnSet;
    }
}
